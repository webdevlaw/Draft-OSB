#### CHAPTER 2

#### PROVIDERS OF USER-TO-USER SERVICES: DUTIES OF CARE


User-to-user services: duties of care

**5 Providers of user-to-user services: duties of care**


(1) Subsections (2) to (6) apply to determine which of the duties set out in this
Chapter and Chapter 3 apply in relation to a particular regulated user-to-user
service.


(2) All providers of regulated user-to-user services must comply with the
following duties in relation to each such service—
(a) the illegal content risk assessment duty (see section 7(1)),
(b) each of the illegal content duties (see section 9),
(c) the duty about rights to freedom of expression and privacy set out in
section 12(2),
(d) the duties about reporting and redress set out in—
(i) section 15(2)(a), and
(ii) section 15(3) and (5) so far as relating to subsection (4)(a)(i),
(b)(i) or (ii), (c)(i) or (d)(i) of that section, and
(e) each of the record-keeping and review duties (see section 16).


(3) Additional duties apply to providers of particular kinds of regulated user-to-
user services, as follows.


(4) All providers of regulated user-to-user services that are likely to be accessed by
children must comply with the following duties in relation to each such
service—
(a) each of the children’s risk assessment duties (see section 7(3) and (4)),
(b) each of the duties to protect children’s online safety (see section 10),
and
(c) the duties about reporting and redress set out in—
(i) section 15(2)(b), and
(ii) section 15(3) and (5) so far as relating to subsection (4)(a)(ii),
(c)(ii) or (d)(ii) of that section.


(5) All providers of Category 1 services must comply with the following duties in
relation to each such service—
(a) each of the adults’ risk assessment duties (see section 7(6) and (7)),
(b) each of the duties to protect adults’ online safety (see section 11),
(c) the duties about rights to freedom of expression and privacy set out in
section 12(3), (4) and (5),
(d) each of the duties to protect content of democratic importance (see
section 13),
(e) each of the duties to protect journalistic content (see section 14), and
(f) the duties about reporting and redress set out in—
(i) section 15(2)(c), and
(ii) section 15(3) and (5) so far as relating to subsection (4)(a)(iii),
(b)(iii) or (iv), (c)(iii) or (d)(iii) of that section.


(6) All providers of regulated user-to-user services that include a search engine
must comply with the following duties in relation to the search engine of each
such service—
(a) if the service is not likely to be accessed by children, the duties referred
to in section 17(2);
(b) if the service is likely to be accessed by children, the duties referred to
in section 17(2) and (3).


(7) For the meaning of “Category 1 service”, see section 59 (register of categories
of services).

**6 Duties of care: supplementary**


A duty set out in this Chapter which must be complied with in relation to a
user-to-user service extends only to—
(a) the design and operation of the service in the United Kingdom, or
(b) in the case of a duty that is expressed to apply in relation to users of a
service, the design and operation of the service as it affects United
Kingdom users of the service.

Risk assessments

**7 Risk assessment duties**


All services


(1) The “illegal content risk assessment duty” is a duty—
(a) to carry out an illegal content risk assessment at a time set out in section
8,
(b) to keep an illegal content risk assessment up to date, including when
OFCOM make any significant change to a risk profile that relates to
services of the kind in question, and
(c) to carry out a further illegal content risk assessment before making any
significant change to any aspect of the design or operation of a service
to which such an assessment is relevant.


Services likely to be accessed by children


(2) The “children’s risk assessment duties” are the duties set out in subsections (3)
and (4).


(3) A duty—
(a) to carry out a children’s risk assessment at a time set out in section 8,
(b) to keep a children’s risk assessment up to date, including when
OFCOM make any significant change to a risk profile that relates to
services of the kind in question, and
(c) to carry out a further children’s risk assessment before making any
significant change to any aspect of the design or operation of a service
to which such an assessment is relevant.

(4) Where a children’s risk assessment of a service identifies the presence of non-
designated content that is harmful to children, a duty to notify OFCOM of—
(a) the kinds of such content identified, and
(b) the incidence of those kinds of content on the service.


Category 1 services


(5) The “adults’ risk assessment duties” are the duties set out in subsections (6)
and (7).


(6) A duty—
(a) to carry out an adults’ risk assessment at a time set out in section 8,
(b) to keep an adults’ risk assessment up to date, including when OFCOM
make any significant change to a risk profile that relates to services of
the kind in question, and
(c) to carry out a further adults’ risk assessment before making any
significant change to any aspect of the design or operation of a service
to which such an assessment is relevant.


(7) Where an adults’ risk assessment of a service identifies the presence of content
that is harmful to adults, other than priority content that is harmful to adults,
a duty to notify OFCOM of—
(a) the kinds of such content identified, and
(b) the incidence of those kinds of content on the service.


Definitions


(8) An “illegal content risk assessment” of a service of a particular kind means an
assessment to identify, assess and understand such of the following as appear
to be appropriate, taking into account the risk profile that relates to services of
that kind—
(a) the user base;
(b) the level of risk of individuals who are users of the service encountering
the following by means of the service—
(i) terrorism content,
(ii) CSEA content,
(iii) priority illegal content, and
(iv) other illegal content,
taking into account (in particular) algorithms used by the service, and
how easily, quickly and widely content may be disseminated by means
of the service;
(c) the level of risk of harm to individuals presented by illegal content of
different descriptions;
(d) the level of risk of functionalities of the service facilitating the presence
or dissemination of illegal content, identifying and assessing those
functionalities that present higher levels of risk;
(e) the different ways in which the service is used, and the impact that has
on the level of risk of harm that might be suffered by individuals;
(f) the nature, and severity, of the harm that might be suffered by
individuals from the matters identified in accordance with paragraphs
(b) to (e);

(g) how the design and operation of the service (including the business
model, governance and other systems and processes) may reduce or
increase the risks identified.


(9) A “children’s risk assessment” of a service of a particular kind means an
assessment to identify, assess and understand such of the following as appear
to be appropriate, taking into account the risk profile that relates to services of
that kind—
(a) the user base, including the number of users who are children in
different age groups;
(b) the level of risk of children who are users of the service encountering
the following by means of the service—
(i) each kind of primary priority content that is harmful to children
(with each kind separately assessed),
(ii) each kind of priority content that is harmful to children (with
each kind separately assessed), and
(iii) non-designated content that is harmful to children,
giving separate consideration to children in different age groups, and
taking into account (in particular) algorithms used by the service and
how easily, quickly and widely content may be disseminated by means
of the service;
(c) the level of risk of harm to children presented by different descriptions
of content that is harmful to children, giving separate consideration to
children in different age groups;
(d) the level of risk of functionalities of the service facilitating the presence
or dissemination of content that is harmful to children, identifying and
assessing those functionalities that present higher levels of risk,
including functionalities—
(i) enabling adults to search for other users of the service
(including children), and
(ii) enabling adults to contact other users (including children) by
means of the service;
(e) the different ways in which the service is used, and the impact that has
on the level of risk of harm that might be suffered by children;
(f) the nature, and severity, of the harm that might be suffered by children
from the matters identified in accordance with paragraphs (b) to (e),
giving separate consideration to children in different age groups;
(g) how the design and operation of the service (including the business
model, governance and other systems and processes) may reduce or
increase the risks identified.


(10) An “adults’ risk assessment” of a service of a particular kind means an
assessment to identify, assess and understand such of the following as appear
to be appropriate, taking into account the risk profile that relates to services of
that kind—
(a) the user base;
(b) the level of risk of adults who are users of the service encountering the
following by means of the service—
(i) each kind of priority content that is harmful to adults (with each
kind separately assessed), and
(ii) other content that is harmful to adults,
taking into account (in particular) algorithms used by the service, and
how easily, quickly and widely content may be disseminated by means
of the service;
(c) the level of risk of harm to adults presented by different descriptions of
content that is harmful to adults;
(d) the level of risk of functionalities of the service facilitating the presence
or dissemination of content that is harmful to adults, identifying and
assessing those functionalities that present higher levels of risk;
(e) the different ways in which the service is used, and the impact that has
on the level of risk of harm that might be suffered by adults;
(f) the nature, and severity, of the harm that might be suffered by adults
from the matters identified in accordance with paragraphs (b) to (e);
(g) how the design and operation of the service (including the business
model, governance and other systems and processes) may reduce or
increase the risks identified.


(11) In this section references to risk profiles are to risk profiles included in
OFCOM’s guidance about risk assessments (see section 62).

**8 Timing of risk assessment under section 7**


(1) In the case of a regulated user-to-user service which is in operation
immediately before the relevant day, a risk assessment must be carried out
within the period of three months beginning with that day, unless extra time is
allowed by agreement with OFCOM.


(2) In the case of a regulated user-to-user service which begins operating on or
after the relevant day, a risk assessment must be carried out before United
Kingdom users are able to access the service.


(3) In the case of a user-to-user service which, having previously not been a
regulated user-to-user service, becomes a regulated user-to-user service, a risk
assessment must be carried out—
(a) before United Kingdom users are able to access the service, or
(b) if such users were already able to access the service, as soon as
reasonably practicable after the service becomes a regulated service.


(4) In this section “the relevant day” means—
(a) if OFCOM’s report of their risk assessment under section 61 and
OFCOM’s guidance about risk assessments under section 62 are first
published on the same day, that day, or
(b) if that report and that guidance are first published on different days, the
later of those days.


(5) In this section “risk assessment” means a risk assessment under section 7(1)(a),
(3)(a) or (6)(a).


Safety duties

**9 Safety duties about illegal content**


(1) The “illegal content duties” in relation to user-to-user services are the duties set
out in this section.

(2) A duty, in relation to a service, to take proportionate steps to mitigate and
effectively manage the risks of harm to individuals, as identified in the most
recent illegal content risk assessment of the service.


(3) A duty to operate a service using proportionate systems and processes
designed to—
(a) minimise the presence of priority illegal content;
(b) minimise the length of time for which priority illegal content is present;
(c) minimise the dissemination of priority illegal content;
(d) where the provider is alerted by a person to the presence of any illegal
content, or becomes aware of it in any other way, swiftly take down
such content.


(4) A duty to specify in the terms of service how individuals are to be protected
from illegal content, addressing each paragraph of subsection (3).


(5) A duty to ensure that—
(a) the terms of service referred to in subsection (4) are clear and accessible,
and
(b) those terms of service are applied consistently.


(6) In determining whether a step, system or process is proportionate for the
purposes of this section, the following must be taken into account—
(a) all the findings of the most recent illegal content risk assessment
(including as to levels of risk and as to nature, and severity, of potential
harm to individuals), and
(b) the size and capacity of the provider of a service.


(7) In this section “illegal content risk assessment” has the meaning given by
section 7(8).


(8) See also, in relation to duties under this section, section 12(2) (duties about
rights to freedom of expression and privacy).

**10 Safety duties for services likely to be accessed by children**


(1) The “duties to protect children’s online safety” in relation to user-to-user
services are the duties set out in this section.


(2) A duty, in relation to a service, to take proportionate steps to—
(a) mitigate and effectively manage the risks of harm to children in
different age groups, as identified in the most recent children’s risk
assessment of the service, and
(b) mitigate the impact of harm arising to children in different age groups
from content that is harmful to children present on the service.


(3) A duty to operate a service using proportionate systems and processes
designed to—
(a) prevent children of any age from encountering, by means of the service,
primary priority content that is harmful to children;
(b) protect children in age groups judged to be at risk of harm from other
content that is harmful to children (or from a particular kind of such
content) from encountering it by means of the service.


(4) A duty to specify in the terms of service—

(a) how children of any age are to be prevented from encountering
primary priority content that is harmful to children (with each kind of
primary priority content separately covered);
(b) how children in age groups judged to be at risk of harm from priority
content that is harmful to children (or from a particular kind of such
content) are to be protected from encountering it, where they are not
prevented from doing so (with each kind of priority content separately
covered);
(c) how children in age groups judged to be at risk of harm from non-
designated content that is harmful to children (or from a particular kind
of such content) are to be protected from encountering it, where they
are not prevented from doing so.


(5) A duty to ensure that—
(a) the terms of service referred to in subsection (4) are clear and accessible,
and
(b) those terms of service are applied consistently.


(6) In determining whether a step, system or process is proportionate for the
purposes of this section, the following must be taken into account—
(a) all the findings of the most recent children’s risk assessment (including
as to levels of risk and as to nature, and severity, of potential harm to
children), and
(b) the size and capacity of the provider of a service.


(7) So far as a duty in this section relates to non-designated content that is harmful
to children, the duty is to be taken to extend only to addressing risks of harm
from the kinds of such content that have been identified in the most recent
children’s risk assessment (if any have been identified).


(8) References in subsections (3)(b) and (4)(b) and (c) to children in age groups
judged to be at risk of harm from content that is harmful to children are
references to children in age groups judged to be at risk of such harm as
assessed by the provider of a service in the most recent children’s risk
assessment of the service.


(9) The duties in this section extend only to such parts of a service as it is possible
for children to access.
Section 26(3) applies for the purposes of this subsection as it applies for the
purposes of an assessment under section 26.


(10) In this section “children’s risk assessment” has the meaning given by section
7(9).


(11) See also, in relation to duties under this section, section 12(2) (duties about
rights to freedom of expression and privacy).

**11 Safety duties protecting adults: Category 1 services**


(1) The “duties to protect adults’ online safety” in relation to user-to-user services
are the duties set out in this section.


(2) A duty to specify in the terms of service—
(a) how priority content that is harmful to adults is to be dealt with by the
service (with each such kind of priority content separately covered),
and

(b) how other content that is harmful to adults, of a kind that has been
identified in the most recent adults’ risk assessment (if any kind of such
content has been identified), is to be dealt with by the service.


(3) A duty to ensure that—
(a) the terms of service referred to in subsection (2) are clear and accessible,
and
(b) those terms of service are applied consistently.


(4) In this section “adults’ risk assessment” has the meaning given by section 7(10).


(5) See also, in relation to duties under this section, section 12(2) (duties about
rights to freedom of expression and privacy).


Freedom of expression etc

**12 Duties about rights to freedom of expression and privacy**


(1) The duties about rights to freedom of expression and privacy in relation to
user-to-user services, as referred to in section 5, are the duties set out in this
section.


All services


(2) A duty to have regard to the importance of—
(a) protecting users’ right to freedom of expression within the law, and
(b) protecting users from unwarranted infringements of privacy,
when deciding on, and implementing, safety policies and procedures.


Category 1 services


(3) A duty—
(a) when deciding on safety policies and procedures, to carry out an
assessment of the impact that such policies or procedures would have
on—
(i) the protection of users’ right to freedom of expression within
the law, and
(ii) the protection of users from unwarranted infringements of
privacy; and
(b) to carry out an assessment of the impact of adopted safety policies and
procedures on the matters mentioned in paragraph (a)(i) and (ii).


(4) A duty to—
(a) keep an impact assessment up to date, and
(b) publish impact assessments.


(5) A duty to specify in the terms of service, or in a publicly available statement,
the positive steps that the provider has taken in response to an impact
assessment to—
(a) protect users’ right to freedom of expression within the law, and
(b) protect users from unwarranted infringements of privacy.


Definitions


(6) In this section—

“impact assessment” means an impact assessment under subsection (3);
“safety policies and procedures” means policies and procedures designed
to secure compliance with—
(a) any of the Chapter 2 safety duties, or
(b) any of the duties set out in section 15 (reporting and redress).

**13 Duties to protect content of democratic importance: Category 1 services**


(1) The “duties to protect content of democratic importance” in relation to user-to-
user services are the duties set out in this section.


(2) A duty to operate a service using systems and processes designed to ensure
that the importance of the free expression of content of democratic importance
is taken into account when making decisions about—
(a) how to treat such content (especially decisions about whether to take it
down or restrict users’ access to it), and
(b) whether to take action against a user generating, uploading or sharing
such content.


(3) A duty to ensure that the systems and processes mentioned in subsection (2)
apply in the same way to a diversity of political opinion.


(4) A duty to specify in the terms of service the policies and processes that are
designed to take account of the principle mentioned in subsection (2),
including, in particular, how that principle is applied to decisions mentioned
in that subsection.


(5) A duty to ensure that—
(a) the terms of service referred to in subsection (4) are clear and accessible,
and
(b) those terms of service are applied consistently.


(6) For the purposes of this section content is “content of democratic importance”,
in relation to a user-to-user service, if—
(a) the content is—
(i) news publisher content in relation to that service, or
(ii) regulated content in relation to that service; and
(b) the content is or appears to be specifically intended to contribute to
democratic political debate in the United Kingdom or a part or area of
the United Kingdom.


(7) In this section, the reference to “taking action” against a user is to giving a
warning to a user, or suspending or banning a user from using a service, or in
any way restricting a user’s ability to use a service.


(8) For the meaning of “news publisher content” and “regulated content”, see
section 39.

**14 Duties to protect journalistic content: Category 1 services**


(1) The “duties to protect journalistic content” in relation to user-to-user services
are the duties set out in this section.


(2) A duty to operate a service using systems and processes designed to ensure
that the importance of the free expression of journalistic content is taken into
account when making decisions about—

(a) how to treat such content (especially decisions about whether to take it
down or restrict users’ access to it), and
(b) whether to take action against a user generating, uploading or sharing
such content.


(3) A duty, in relation to a decision by a provider to take down content or to
restrict access to it, to make a dedicated and expedited complaints procedure
available to a person who considers the content to be journalistic content and
who is—
(a) the user who generated, uploaded or shared the content on the service,
or
(b) the creator of the content (see subsection (11)).


(4) A duty to make a dedicated and expedited complaints procedure available to
users of a service in relation to a decision by the provider of the service to take
action against a user because of content generated, uploaded or shared by the
user which the user considers to be journalistic content.


(5) A duty to ensure that—
(a) if a complaint about a decision mentioned in subsection (3) is upheld,
the content is swiftly reinstated on the service;
(b) if a complaint about a decision mentioned in subsection (4) is upheld,
the action against the user is swiftly reversed.


(6) A duty to specify in the terms of service—
(a) by what methods content present on the service is to be identified as
journalistic content;
(b) how the importance of the free expression of journalistic content is to
be taken into account when making decisions mentioned in subsection
(2);
(c) the policies and processes for handling complaints in relation to
content which is, or is considered to be, journalistic content.


(7) A duty to ensure that—
(a) the terms of service referred to in subsection (6) are clear and accessible,
and
(b) those terms of service are applied consistently.


(8) For the purposes of this section content is “journalistic content”, in relation to
a user-to-user service, if—
(a) the content is—
(i) news publisher content in relation to that service, or
(ii) regulated content in relation to that service;
(b) the content is generated for the purposes of journalism; and
(c) the content is UK-linked.


(9) For the purposes of this section content is “UK-linked” if—
(a) United Kingdom users of the service form one of the target markets for
the content (or the only target market), or
(b) the content is or is likely to be of interest to a significant number of
United Kingdom users.


(10) In this section references to “taking action” against a user are to giving a
warning to a user, or suspending or banning a user from using a service, or in
any way restricting a user’s ability to use a service.

(11) In this section the reference to a person who is the “creator” of content is a
reference to any of the following—
(a) in the case of news publisher content, the recognised news publisher in
question;
(b) an individual who—
(i) created the content, and
(ii) is in the United Kingdom;
(c) an entity which—
(i) created the content, and
(ii) is incorporated or formed under the law of any part of the
United Kingdom.


(12) For the meaning of “news publisher content”, “regulated content” and
“recognised news publisher”, see sections 39 and 40.


User reporting and redress

**15 Reporting and redress duties**


(1) The duties about reporting and redress in relation to user-to-user services, as
referred to in section 5, are the duties set out in this section.


(2) A duty to operate a service using systems and processes that allow users and
affected persons to easily report content of the following kinds—
(a) content which they consider to be illegal content;
(b) content, present on a part of a service that it is possible for children to
access, which they consider to be content that is harmful to children;
(c) content which they consider to be content that is harmful to adults.


(3) A duty to operate a complaints procedure in relation to a service that—
(a) allows for complaints of the kinds mentioned in subsection (4) to be
made,
(b) provides for appropriate action to be taken by the provider of the
service in response to such complaints, and
(c) is easy to access, easy to use (including by children) and transparent.


(4) The kinds of complaints are—
(a) complaints by users and affected persons about content of the
following kinds—
(i) content present on a service which they consider to be illegal
content;
(ii) content, present on a part of a service that it is possible for
children to access, which they consider to be content that is
harmful to children;
(iii) content present on a service which they consider to be content
that is harmful to adults;
(b) complaints by users and affected persons if they consider that—
(i) the provider is not complying with a Chapter 2 safety duty that
applies in relation to the service;
(ii) the provider is not complying with the duty set out in section
12(2);

(iii) the provider is not complying with any of the duties set out in
section 12(3) to (5);
(iv) the provider is not complying with any of the duties set out in
section 13 or 14;
(c) complaints by a user who has generated, uploaded or shared content
on a service if the provider—
(i) takes down that content because the provider considers that it
is illegal content;
(ii) takes down that content or restricts access to it because the
provider considers that it is content that is harmful to children;
(iii) takes down that content or restricts access to it because the
provider considers that it is content that is harmful to adults;
and
(d) complaints by a user if the provider has given a warning to the user,
suspended or banned the user from using the service, or in any other
way restricted the user’s ability to use the service, as a result of content
generated, uploaded or shared by the user which the provider
considers to be—
(i) illegal content;
(ii) content that is harmful to children;
(iii) content that is harmful to adults.


(5) A duty to make the policies and procedures that govern the handling and
resolution of complaints as mentioned in subsection (4) publicly available and
easily accessible (including to children).


(6) Section 26(3) (access by children to a service) applies for the purposes of
subsections (2)(b) and (4)(a)(ii) as it applies for the purposes of an assessment
under section 26.


(7) In this section “affected person” means a person, other than a user of the service
in question, who is in the United Kingdom and who is—
(a) the subject of the content,
(b) a member of a class or group of people with a certain characteristic (or
combination of characteristics) targeted by the content,
(c) a parent of, or other adult with responsibility for, a child who is a user
of the service or is the subject of the content, or
(d) an adult providing assistance in using the service to another adult who
requires such assistance, where that other adult is a user of the service
or is the subject of the content.


(8) See also, in relation to duties under this section, section 12(2) (duties about
rights to freedom of expression and privacy).


Record-keeping and review

**16 Record-keeping and review duties**


(1) The “record-keeping and review duties” in relation to user-to-user services are
the duties set out in this section.


(2) A duty to make and keep a written record of every risk assessment carried out
under section 7.

(3) A duty to make and keep a written record of any steps taken to comply with a
relevant duty other than steps which—
(a) are described in a code of practice and recommended for the purposes
of compliance with the duty in question, and
(b) apply in relation to the provider and the service in question.


(4) A duty to review compliance with the relevant duties in relation to a service—
(a) regularly, and
(b) as soon as reasonably practicable after making any significant change
to any aspect of the design or operation of the service.


(5) Where OFCOM consider it to be appropriate, OFCOM may, in relation to a
particular user-to-user service, exempt the provider of that service from—
(a) the duty set out in subsection (2),
(b) the duty set out in subsection (3), or
(c) the duties set out in subsections (2) and (3).


(6) OFCOM must publish details of any exemption under subsection (5).


(7) In this section—
“code of practice” means a code of practice published under section 34;
“relevant duties” means—
(a) the Chapter 2 safety duties,
(b) the duties set out in section 13 (content of democratic
importance),
(c) the duties set out in section 14 (journalistic content), and
(d) the duties set out in section 15 (reporting and redress).


##### EXPLANATORY NOTES FOR THIS SECTION

Chapter 2: Providers of user-to-user services: Duties of care

Clause 5: Providers of user-to-user services: duties of care

This clause determines which of the duties set out in Part 2 apply to which regulated
user-to-user services. The duties on search services are set out in the following
chapter.
Subsection (2) lists the duties that all regulated user-to-user service providers must
comply with, as follows:
a. the illegal content risk assessment duty in clause 7;
b. the illegal content duties in clause 9;
c. the duty about rights to freedom of expression and privacy in clause 12;
d. the applicable duties about reporting and redress set out in clause 15; and
e. the record-keeping and review duties in clause 16.
Subsection (3) provides that certain additional duties will apply to providers of
particular kinds of regulated user-to-user services as detailed in subsections (4) to
(6).
Subsection (4) lists the additional duties that will be imposed upon all providers of a
user-to-user service which is likely to be accessed by children (as is determined by
the service provider in accordance with clause 26). These are:
a. the duties to undertake children’s risk assessments in clause 7;
b. the duties to protect children’s safety online in clause 10; and
c. the relevant reporting and redress duties in clause 15 which apply in relation
to content that is considered to be harmful to children.
Subsection (5) lists the additional duties to be imposed on providers of Category 1
services (the designation as a Category 1 service is determined by OFCOM’s
assessment under the provisions in clause 59). These are:
These explanatory notes relate to the Online Safety Bill as published in draft on 12 May 2021 (Bill CP 405)
a. the duties to undertake adult risk assessments in clause 7;
b. the duties to protect adults’ online safety in clause 11;
c. the duties to protect users’ rights in clause 12;
d. the duties to protect content of democratic importance and journalistic content
in clauses 13 and 14; and
e. the relevant reporting and redress duties set out in clause 15 relating to
content that is harmful to adults and to the duties to protect content of
democratic importance and journalistic content.
Where a user-to-user service includes a search engine, under subsection (6) the
provider of the service must comply with the additional duties in relation to the search
engine:
a. If the service is not likely to be accessed by children, then it must comply with
the duties with regards to illegal content in clause 17(2).
b. If the service is likely to be accessed by children, then it must also comply
with the duties with regards to illegal content and protecting children in clause
17(3).
Clause 6: Duties of care: supplementary

This clause makes it clear that the duties in this Chapter apply only to the design and
operation of a user-to-user service as it affects users and others (such as individuals
affected by content on services they do not themselves use) in the United Kingdom.
Clause 7: Risk assessment duties: user-to-user services

This clause sets out the risk assessment duties on regulated user-to-user services.
These duties relate to assessing the risks arising from (i) illegal content, (ii) harm to
children from content that is not illegal, and (iii) harm to adults from content that is not
illegal. Not all regulated user-to-user service providers will have to risk assess
against all such types of content.
Subsection (1) sets out the risk assessment duties in relation to illegal content that all
regulated user-to-user service providers must comply with. These are:
a. to undertake an illegal content risk assessment;
b. to keep that risk assessment up to date; and
c. to update it before the service makes a significant change.
Subsection (3) relates to the children’s risk assessment duties that regulated
providers of services which are likely to be accessed by children (as assessed in
accordance with clause 26) must adhere to. These are:
a. to undertake a children’s risk assessment;
These explanatory notes relate to the Online Safety Bill as published in draft on 12 May 2021 (Bill CP 405)
b. to keep that risk assessment up to date; and
c. to update it before the service makes a significant change.
Subsection (4) requires service providers to notify OFCOM about content they
identify that is harmful to children that is not of a type specified in secondary
legislation as primary priority or priority content that is harmful to children, as well as
how often such content appears on the service.
Subsection (6) sets out the risk assessment duties for content that is legal but
harmful to adults which providers of Category 1 services must comply with for those
services. These are:
a. to undertake an adults’ risk assessment;
b. to keep that risk assessment up to date; and
c. to update it before the service makes a significant change.
Subsection (7) requires service providers to notify OFCOM about content they
identify that is harmful to adults but which is not of a type specified in secondary
legislation as priority content that is harmful to adults, as well as how often such
content appears on the service.
Subsections (8) to (10) then define what the risk assessments for each of the three
types of content should cover, and require that service providers must identify,
assess and understand a number of factors (as appropriate) as set out in those
subsections.
OFCOM will have a duty to issue guidance about risk assessments to assist
providers of different types of services how to carry out their risk assessments: see
clause 62. This will ensure providers have, for example, sufficient clarity about what a
proportionate risk assessment looks like for their type of service and what would
constitute a significant change that would require an updated risk assessment.
Clause 8: Timing of risk assessment under section 7

This clause sets out the time periods within which a regulated user-to user service
provider must carry out the risk assessments referred to above. The time period will
start to run from the day on which OFCOM publishes its report on its risk assessment
under clause 61 and its guidance about risk assessments under clause 62, or if they
are published on different days, whichever is the later of those days: see subsection
(4). This is referred to as the “relevant day”.
If the regulated user-to-user service was already operating immediately before the
relevant day, they must carry out their risk assessments within three months unless
they agree extra time with OFCOM: subsection (1). If the service provider begins
operating after the relevant day then the risk assessment must be carried out before
UK users are able to access the service: subsection (2). In the case of a user-to-user
service which was not previously regulated but then becomes regulated, the relevant
risk assessments must be carried out before users in the UK can access the service
These explanatory notes relate to the Online Safety Bill as published in draft on 12 May 2021 (Bill CP 405)
or, if UK users can already access the service, as soon as possible after the service
becomes regulated: sub-section (3).
Clause 9: Safety duties about illegal content

This clause sets out the duties on user-to-user services with regards to illegal
content. As established by clause 5, all user-to-user services must comply with these
duties.
Subsection (2) provides for a duty to take proportionate steps to reduce and manage
the risk of harm to individuals identified in the illegal content risk assessment carried
out under clause 7.
Subsection (3) requires service providers to use proportionate systems and
processes designed to:
a. Minimise the presence of priority illegal content on the service in the first
place.
b. Where priority illegal content is uploaded, to minimise the time for which it is
present and its dissemination.
c. Remove illegal content the service provider is made aware of, or becomes
aware of, as soon as possible.
Subsections (4) and (5) impose obligations on providers to state in their terms of
service how individuals are to be protected from illegal content, and then to ensure
these terms are clear and accessible and applied consistently.
Subsection (6) specifies that whether steps, systems and processes in this case are
proportionate will be determined by the levels of risk identified in the risk assessment
and the service provider’s size and capacity.
Subsection (8) links the duties about users’ rights to freedom of expression and
privacy in clause 12 to the illegal content safety duty in this clause.
Clause 10: Safety duties for services likely to be accessed by children

This clause sets out the duties on user-to-user services with regard to content that is
harmful to children but is not illegal. As established in clause 5, user-to-user services
that are likely to be accessed by children must comply with these duties.
Subsection (2) provides for a duty on services to take proportionate steps:
a. to manage the risk of harm to children in different age groups from risks
identified in the children’s risk assessment as carried out under clause 7.
b. to mitigate the impact of harm to children in different age groups from content
that is harmful to children.
Subsection (3) requires service providers to use proportionate systems and
processes, which are designed to:
These explanatory notes relate to the Online Safety Bill as published in draft on 12 May 2021 (Bill CP 405)
a. Prevent children of any age from accessing primary priority content on their
service, as defined in regulations to be made under clause 45.
b. Protect children in age groups which are judged to be at risk from other
content that is harmful to children (being priority content as defined in
regulations made under clause 45 and other content that satisfies the
definition of content that is harmful to children) on their service.
Subsections (4) and (5) require providers to state in their terms of service how
children are being prevented from encountering primary priority content on their
service and how children are to be protected from encountering priority content that
is harmful for children on their service, as set out in subsection (3). It also requires
providers to set out how children are to be protected from encountering other content
that would satisfy the definition of harmful to children. Providers must then ensure
these terms are clear and accessible, and applied consistently.
Subsection (6) specifies that whether steps, systems and processes in this case are
proportionate is determined by the levels of risk identified in the risk assessment and
the service provider’s size and capacity.
Subsection (7) makes clear that services are only required to fulfil the duty in this
section in relation to non-designated content (i.e. neither primary priority content nor
priority content, but content that would satisfy the definition of being harmful to
children) if risks from non-designated content have been identified in the most recent
children’s risk assessment.
Subsection (8) explains that references in this clause to children judged to be in age
groups at risk of harm from content that is harmful to children, are to be read as
being those who have been assessed as such by the provider in their most recent
children’s risk assessment.
Subsection (9) clarifies that the duties in this section to protect children only extend to
those parts of the service which it is possible for children to access, in line with the
assessment on children’s access set out in clause 26. For example, a service could
have robust systems and processes, such as effective age verification measures,
that ensure children are not normally able to access a part of the service.
Subsection (11) links the duties about users’ rights to freedom of expression and
privacy in clause 12 to the safety duties for services likely to be accessed by children.
Clause 11: Safety duties protecting adults: Category 1 services

This clause sets out the duties on Category 1 service providers with regards to
content that is harmful to adults.
Subsection (2) provides for obligations to state in a provider’s terms of service how
content that is harmful to adults (as defined in clause 46) will be treated by the
service provider on the Category 1 service in question. Subsection (3) requires that
these terms of service must be both clear and accessible, and applied consistently.
These explanatory notes relate to the Online Safety Bill as published in draft on 12 May 2021 (Bill CP 405)
Subsection (5) links the duties about users’ rights to freedom of expression and
privacy in clause 12 to the safety duty for content that is harmful to adults.
Clause 12: Duties about rights to freedom of expression and privacy

This clause sets out the duties about protecting users’ rights in relation to user-to-
user services.
Subsection (2) states that when designing and implementing their safety policies and
procedures, all providers of regulated user-to-user services must have regard to the
importance of protecting users’ rights to freedom of expression and protecting users
from unwarranted infringements of privacy.
Subsection (3) states that Category 1 service providers, additionally, have a duty to
carry out an assessment of the impact that their safety policies and procedures will
have on users’ rights to freedom of expression and privacy. This applies to safety
policies and procedures that they are considering and those that they have adopted
on the Category 1 service.
Subsection (4) requires such service providers to publish these impact assessments
and to keep them up to date.
Subsection (5) puts a duty on Category 1 service providers to state (either in their
terms of service or in a publicly available statement) what steps they have taken with
regard to the Category 1 service in question in response to the impact assessment
set out in subsection (3).
References to users' rights to freedom of expression within the law include common
law rights.
Subsection (6) confirms that the safety policies and procedures are those which are
designed to ensure compliance with any of the safety duties and the reporting and
redress duties in clause 15.
Clause 13: Duties to protect content of democratic importance: Category 1 services

This clause sets out the duties on providers of Category 1 services with regard to
protecting content of democratic importance on those services.
Subsection (2) requires providers of Category 1 services to put in place systems and
processes which ensure that the importance of the free expression of content of
democratic importance is taken into account when making decisions on how to treat
such content (especially decisions about taking it down or restricting users’ access to
it), and on how to treat users who generate or share it on their services.
Subsection (3) requires providers of Category 1 services to ensure that those
systems and processes apply in the same way to a diversity of political opinion.
Subsection (4) requires providers of Category 1 services to specify in their terms of
service how their policies and processes (particularly in relation to moderation
decisions) are designed to take account of the importance of the free expression of
content of democratic importance.
These explanatory notes relate to the Online Safety Bill as published in draft on 12 May 2021 (Bill CP 405)
Subsection (5) states that those terms of service must be clear and accessible, and
applied consistently.
Subsection (6) defines “content of democratic importance” as news publisher content
or regulated content which is, or appears to be, specifically intended to contribute to
democratic political debate in the UK or in any part or area of the UK. Examples of
such content would be content promoting or opposing government policy and content
promoting or opposing a political party.
Subsection (7) clarifies what is meant by ‘taking action’ against a user, while
subsection (8) signposts the definitions of the terms “news publisher content” and
“regulated content” in clause 39.
Clause 14: Duties to protect journalistic content: Category 1 services

This clause sets out the duties on providers of Category 1 services with regard to
protecting journalistic content on those services.
Subsection (2) requires providers of Category 1 services to put in place systems and
processes which ensure that the importance of the free expression of journalistic
content is taken into account when making decisions on how to treat such content,
and on how to treat users who generate or share it on their services
Subsection (3) requires a provider of Category 1 services to create a dedicated
complaints procedure for users who generate, upload or share what they consider to
be journalistic content on the service and creators of journalistic content, in relation to
decisions by that provider to take down or restrict access to such content.
Subsection (4) requires a provider of Category 1 services to create a
dedicated complaints procedure for users in relation to a decision by that provider to
take action against a user because of content shared, uploaded or generated by the
user which the user considers to be journalistic content.
Subsection (5) requires providers of Category 1 services to act swiftly to
reinstate journalistic content and to reverse actions taken against users who have
generated or shared journalistic content, where complaints under subsections (3) and
(4) are upheld.
Subsection (6) imposes a duty on providers of Category 1 services to specify
in their terms of service how they will identify journalistic content, how they will take
into account the importance of the free expression of journalistic content when taking
moderation decisions, and their policies and processes for handling complaints
concerning journalistic content.
Subsection (7) provides that those terms of service must be clear and
accessible, and applied consistently.
Subsection (8) defines “journalistic content” as content that is generated for
the purposes of journalism, and which is ‘UK-linked’. Journalistic content
encompasses regulated content and news publisher content. Subsection (9) defines
the term “UK-linked”.
These explanatory notes relate to the Online Safety Bill as published in draft on 12 May 2021 (Bill CP 405)
Subsection (10) clarifies what is meant by ‘taking action’ against a user and
subsection (11) defines what is meant by the “creator” of journalistic content, while
(12) signposts the definitions of the terms “news publisher content”, “regulated
content” and “recognised news publisher” in clause 39.
Clause 15: Reporting and redress duties

This clause sets out the user reporting and redress mechanisms, which apply
to particular regulated user-to-user services as set out in clause 5.
Subsection (2) places a duty on services to have systems and processes in
place that allow users or affected persons (as defined in subsection (7)) to report
content that the user or affected person considers to be (a) illegal, (b) harmful to
children (where that content can be accessed by a child), and (c) harmful to adults.
All regulated providers must have systems in place for users or affected persons to
report (a) illegal content, but beyond that, a service provider must only comply with
the reporting duties that apply in relation to the content that is regulated on their
service. For example, only providers of Category 1 services will have a duty to have
a reporting mechanism for content that is harmful to adults on the Category 1 service
in question, and only those services that are likely to be accessed by children will
have reporting and redress duties in respect of content that is harmful to children.
This is set out in clause 5.
Subsection (3) places a duty on the service to have a complaints procedure
that is easy to access, easy to use and transparent, and that provides for the service
provider to take appropriate action in response to the complaints set out in
subsection (4). Where applicable, the complaints procedure must also be easy to
access and use including for children who may wish to complain.
Subsection (4) outlines the kinds of complaints that subsection (3) applies to.
This includes complaints about regulated content, complaints about a provider not
complying with their safety duties or their duties in relation to freedom of expression
and privacy, democratic content and journalistic content. This also includes
complaints about action a provider has taken in relation to regulated content such as
taking down or restricting access to that content and suspending or banning a user
from using the service as a result of that content.
Subsection (5) places a duty on services to make their complaints policies
and procedures publicly available and easy to access. This is to ensure that users
and affected persons can easily find the complaints policies and procedures and that
the process is transparent.
Subsection (6) provides that clause 26(3), which sets out when a provider is
entitled to conclude that it is not possible for children to access a service, or part of a
service, also applies for the purposes of the provisions in this clause which refer to
the parts of a service which it is possible for children to access.
Subsection (7) defines an ‘affected person’ as someone other than a user of
the service who is in the United Kingdom and is:
These explanatory notes relate to the Online Safety Bill as published in draft on 12 May 2021 (Bill CP 405)
a. The subject of the content.
b. A member of a class or group of people with a certain characteristic targeted
by the content.
c. A parent or other adult with responsibility for a child who is the user of the
service or the subject of that content.
d. A person who is providing assistance in using the service to another adult
who requires such assistance and who is a user of the service or the subject
of the content.
Subsection (8) refers to the duties in respect of rights to freedom of
expression and privacy in relation to the duties under this clause
Clause 16: Record-keeping and review duties

This clause sets out the record-keeping and review obligations that apply to
regulated user-to-user services.
Subsection (2) puts a duty on providers to keep a written record of the risk
assessments carried out under clause 7.
Subsection (3) requires providers to keep a written record of any steps they
have taken to comply with the relevant duties listed in subsection (7) that are not
provided for in the codes of practice which apply to that provider and service in
question. This requirement does not apply where a service provider has followed
steps set out in a code of practice.
Subsection (4) requires providers to review compliance with the relevant
duties regularly and after making any significant change to the design and operation
of their service.
Subsection (5) provides OFCOM with the ability to exempt certain providers
from the need to keep written records. It is anticipated that this power could be used
where there are small, low risk services. Under subsection (6) OFCOM must publish
the details of any such exemptions.

#### DELEGATED POWERS MEMORANDUM

PART 2: PROVIDERS OF REGULATED SERVICES: DUTIES OF CARE
CHAPTER 2: PROVIDERS OF USER-TO-USER SERVICES: DUTIES OF CARE
Clause 16(5): Record-keeping and review duties

Power conferred on: OFCOM
Power exercised by: Decision
Parliamentary procedure: None

Context and purpose

Clause 16 requires providers of user-to-user services to keep records of and review
compliance with their safety duties, duties to protect users rights, user reporting and
redress duties and, in the case of Category 1 Services, duties to protect journalistic
content and content of democratic importance. Those record-keeping and review
duties are:
○ To make and keep a written record of every risk assessment carried out
under clause 7 (subsection (2));
○ To make and keep a written record of any steps taken to comply with a
relevant duty other than recommended steps set out in a code of practice
(subsection (3)); and
○ To review compliance with the duties listed above regularly, and as soon as
reasonably practicable after making any significant change to any aspect of
the design or operation of the service (subsection (4)).
The record-keeping and review duties set out above are intended to ensure effective
scrutiny of regulated services and are central to ensuring that regulated services
operate transparently and in compliance with the regulatory framework.
However, the range of regulated user-to-user service providers is extensive and such
comprehensive duties may be excessive in specific cases, for example for smaller,
lower-risk services. Under subsection (5), where OFCOM consider it to be
appropriate, OFCOM may, in relation to a particular user-to-user service, exempt the
provider of that service from the duty set out in subsection (2), the duty set out in
subsection (3), or both the duties set out in subsections (2) and (3). Under
subsection (6) OFCOM must publish details of any such exemption.
Justification for the power

This power enabling OFCOM to exempt a particular service from the record-keeping
and review duties will allow OFCOM to remain responsive and adapt to changes in
the regulatory landscape. It will help to ensure that the regime remains effective and
proportionate.
Delegating this power to OFCOM will allow exemptions to be provided when the
framework is operational and OFCOM can use its full resources and expertise to
assess what expectations are proportionate for particular services to comply with.
This power is limited and its scope is clearly set out in primarily legislation. This power
will not allow services to be removed from scope entirely, nor will it reduce the steps
companies must take to comply with the wider requirements of the regulatory
framework. It only allows the record-taking steps companies must undertake to be
reduced where it is considered appropriate by OFCOM.

Justification for the procedure

OFCOM must publish the details of any exemption granted under subsection (5). This
will ensure that its exercise of this power can be scrutinised.
Since these provisions are concerned with operational and administrative matters in
the context of how the regulator intends to use its powers, the provision to be made is
administrative rather than legislative in character. Therefore, no Parliamentary
procedure is considered necessary.
